Instruction

Our AI works with Ollama local api.
Install Ollama and turn on `Expose Ollama to the network` to make it work.
The default api of Ollama is http://localhost:11434/api/generate/

`The files location depends on your Ollama Install Location`
Put model file to `X:\YOUR USERNAME\.ollama\models\blobs`
Config file to `X:\YOUR USERNAME\.ollama\models\manifests\registry.ollama.ai\library\uon` (IF THERE IS NO THIS FOLDER YOU NEED TO CREATE YOURSELF)

Then Run Ollama to active api